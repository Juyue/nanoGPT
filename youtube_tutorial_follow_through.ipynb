{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "head_dim = 128\n",
    "emb_dim = 256\n",
    "learning_rate = 1e-3\n",
    "num_step_every_eval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "print(f\"length of data: {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters: 65\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "vocab_size = len(set(chars))\n",
    "print(f\"number of unique characters: {vocab_size}\")\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world [46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42] hello world\n"
     ]
    }
   ],
   "source": [
    "# build a encoder and decoder; \n",
    "# step 0: build a mapping\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "# step 1: write encode/decode functions \n",
    "encode = lambda s: [stoi[ch] for ch in s] # string to int\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # int to string\n",
    "# step 3: test the functions\n",
    "input_str = \"hello world\"\n",
    "tokens = encode(input_str)\n",
    "decoded_token = decode(tokens)\n",
    "print(input_str, tokens, decoded_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31373, 995]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "## BPE\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\") # subword tokenizer\n",
    "enc.n_vocab\n",
    "print(enc.encode(input_str))\n",
    "print(enc.decode(enc.encode(input_str)))\n",
    "# trade off between code-book size and sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
      "torch.Size([1115394]) torch.int64 cpu\n"
     ]
    }
   ],
   "source": [
    "# tokenize the entire shakespear text\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])\n",
    "print(data.shape, data.dtype, data.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(len(data) * 0.9)\n",
    "train_data = data[:n_train]\n",
    "val_data = data[n_train:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context 0: , target 0: i\n",
      "context 1: F, target 1: r\n",
      "context 2: Fi, target 2: s\n",
      "context 3: Fir, target 3: t\n",
      "context 4: Firs, target 4:  \n",
      "context 5: First, target 5: C\n",
      "context 6: First , target 6: i\n",
      "context 7: First C, target 7: t\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    context = [c.item() for c in x[:i]]\n",
    "    target = y[i].item()\n",
    "    print(f\"context {i}: {decode(context)}, target {i}: {decode([target])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape torch.Size([4, 8]), target shape torch.Size([4, 8])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "def get_batch(split, batch_size=4, block_size=8):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(0, data.size(0) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(f\"input shape {xb.shape}, target shape {yb.shape}\")\n",
    "print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "#     print(\"-\" *80)\n",
    "#     print(f\"batch {i}\")\n",
    "#     for j in range(block_size):\n",
    "#         context = [c.item() for c in xb[i, :j]]\n",
    "#         target = yb[i, j].item()\n",
    "#         print(f\"context {i}: {decode(context)}, target {i}: {decode([target])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 BigramLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/nanoGPT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train_loss 4.309607028961182, val_loss 4.257331863045692\n",
      "step 100, train_loss 3.2973234462738037, val_loss 2.833229571580887\n",
      "step 200, train_loss 2.7033352065086365, val_loss 2.654280722141266\n",
      "step 300, train_loss 2.5845407748222353, val_loss 2.595935918390751\n",
      "step 400, train_loss 2.5638115072250365, val_loss 2.566488765180111\n",
      "step 500, train_loss 2.5247896695137024, val_loss 2.5465972274541855\n",
      "step 600, train_loss 2.5131516671180725, val_loss 2.546621583402157\n",
      "step 700, train_loss 2.511800954341888, val_loss 2.539495125412941\n",
      "step 800, train_loss 2.4942449593544005, val_loss 2.53522689640522\n",
      "step 900, train_loss 2.490491473674774, val_loss 2.5258804038167\n"
     ]
    }
   ],
   "source": [
    "# build a bigram model P(x_t | x_{t-1}), or P(y | x)\n",
    "class BigramLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # (batch_size, block_size, embedding_dim)\n",
    "        x = self.fc(x) # (batch_size, block_size, vocab_size)\n",
    "        return x\n",
    "\n",
    "    def generate(self, input, max_length=10):\n",
    "        \"\"\"Given input (B, T), extend the sequence to (B, T+max_length).\"\"\"\n",
    "        # loop: logits -> probs -> sample\n",
    "        for _ in range(max_length):\n",
    "            logits = self(input) # (batchsize, block_size, vocab_size)\n",
    "            logits = logits[:, -1, :] # (batch_size, vocab_size), focus on the last token\n",
    "            probs = torch.softmax(logits, -1) # (batch_size, vocab_size), softmax on vocab_size\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input = torch.cat([input, next_token], dim=1)\n",
    "        return input\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "model = BigramLanguageModel(vocab_size, 128)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    \n",
    "@torch.no_grad()\n",
    "def estimate_eval_loss(model, split=\"val\", block_size=block_size):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for _ in range(32):\n",
    "        xb, yb = get_batch(split, batch_size=32, block_size=block_size)\n",
    "        y_pred = model(xb)\n",
    "        loss = loss_fn(y_pred.view(-1, vocab_size), yb.view(-1))\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "# write a training loop\n",
    "n_training_steps = 1000\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for step in range(n_training_steps):\n",
    "    xb, yb = get_batch(\"train\", batch_size=32)\n",
    "    y_pred = model(xb)\n",
    "    # print(y_pred.shape) # (batch_size, block_size, vocab_size)\n",
    "    # print(yb.shape) # (batch_size, block_size)\n",
    "    b, t, c = y_pred.shape\n",
    "    # alternatively, y_pred.view(b*t, c) and yb.view(b*t)\n",
    "    loss = loss_fn(y_pred.view(-1, vocab_size), yb.view(-1)) # view(-1) let the pytorch guess \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss.append(loss.item())\n",
    "    if step % num_step_every_eval == 0:\n",
    "        eval_loss = estimate_eval_loss(model, \"val\")\n",
    "        train_loss_avg = sum(train_loss) / len(train_loss)\n",
    "        train_loss = []\n",
    "        print(f\"step {step}, train_loss {train_loss_avg}, val_loss {eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_xb, initial_yb = get_batch(\"val\")\n",
    "predictions = model.generate(initial_xb, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r:\n",
      "see w s te:\n",
      "Wooulld:\n",
      "hiour her breo ds INIORCHing\n",
      "Poff ghr'souttale il mud tas byouthat thes pldist s out\n"
     ]
    }
   ],
   "source": [
    "print(decode(predictions[1].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 V=W@V # (B, T, C) = (B, T_q, T_k) @ (B, T_k, V), where E = T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[43.0000],\n",
      "        [22.0000],\n",
      "        [31.3333],\n",
      "        [35.2500],\n",
      "        [36.8000]]) tensor([[43.],\n",
      "        [ 1.],\n",
      "        [50.],\n",
      "        [47.],\n",
      "        [43.]])\n"
     ]
    }
   ],
   "source": [
    "# batch_size, block_size = 2, 5\n",
    "xb, yb = get_batch(\"train\", batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "value = xb.unsqueeze(-1).float() # value (B, T, C), C = 1\n",
    "\n",
    "wei = torch.zeros(batch_size, block_size, block_size)\n",
    "tril = torch.tril(torch.ones(block_size, block_size)) # tril is (T_q, T_k)\n",
    "wei = wei.masked_fill(tril==0, float('-inf')) # tril will be broadcasted to (B, T_q, T_k)\n",
    "wei = torch.softmax(wei, -1)\n",
    "\n",
    "next_value = wei @ value # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "print(next_value[0, :, :], value[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttnHead(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads=1, head_dim=16, block_size=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.head_dim = head_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.key = torch.nn.Linear(emb_dim, head_dim, bias=False) # (B, T, C) @ (C, H) -> (B, T, H)\n",
    "        self.query = torch.nn.Linear(emb_dim, head_dim, bias=False)\n",
    "        self.value = torch.nn.Linear(emb_dim, head_dim, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size))) # not parameter, buffer?\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T_k, H)\n",
    "        q = self.query(x) # (B, T_q, H)\n",
    "        v = self.value(x) # (B, T_v, H_v)\n",
    "\n",
    "        wei = torch.einsum(\"bxh,byh->bxy\", q, k) * (self.head_dim ** -0.5) # (B, T_q, T_k)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        wei = torch.softmax(wei, -1)\n",
    "        \n",
    "        # Weighted aggregation of valudes (weighted sum along T_k)\n",
    "        out = torch.einsum(\"bxy,byh->bxh\", wei, v) # (B, T_q, T_k) @ (B, T_k, H) -> (B, T_q, H);\n",
    "        return out\n",
    "    \n",
    "head = SelfAttnHead(emb_dim=128, num_heads=1, head_dim=16, block_size=8)\n",
    "x = torch.zeros(batch_size, block_size, 128)\n",
    "out = head(x)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 4]) torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2, 5, 3)\n",
    "key = torch.nn.Linear(3, 4)\n",
    "k = key(x)\n",
    "q = key(x)\n",
    "print(k.shape, q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"bxh,byh->bxy\", k, q).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
